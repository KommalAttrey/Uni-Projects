{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming application using Spark Structured Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Write code to create a SparkSession, which uses four cores with a proper application name, use the Melbourne timezone, and make sure a checkpoint location has been set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.4.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0 pyspark-shell'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MOTH Streaming Application\") \\\n",
    "    .config(\"spark.master\", \"local[4]\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-Duser.timezone=Australia/Melbourne\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"Australia/Melbourne\") \\\n",
    "    .config(\"spark.sql.streaming.checkpointLocation\", \"path/to/checkpoint\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setCheckpointDir(\"checkpoint_path\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Similar to assignment 2A, write code to define the data schema for the data files, following the data types suggested in the metadata file. Load the static datasets (e.g. customer, product, category) into data frames. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, TimestampType\n",
    "from pyspark.sql.functions import when, count, col, month , first , isnan\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql.functions import year , current_date\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "category_schema = StructType([\n",
    "    StructField(\"category_id\", IntegerType(), True),\n",
    "    StructField(\"cat_level1\", StringType(), True),\n",
    "    StructField(\"cat_level2\", StringType(), True),\n",
    "    StructField(\"cat_level3\", StringType(), True),\n",
    "])\n",
    "customer_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"username\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True), \n",
    "    StructField(\"device_type\", StringType(), True),\n",
    "    StructField(\"device_id\", StringType(), True),\n",
    "    StructField(\"device_version\", StringType(), True),\n",
    "    StructField(\"home_location_lat\", DoubleType(), True),\n",
    "    StructField(\"home_location_long\", DoubleType(), True),\n",
    "    StructField(\"home_location\", StringType(), True),\n",
    "    StructField(\"home_country\", StringType(), True),\n",
    "    StructField(\"first_join_date\", TimestampType(), True), \n",
    "])\n",
    "product_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"baseColour\", StringType(), True),\n",
    "    StructField(\"season\", StringType(), True),\n",
    "    StructField(\"year\", IntegerType(), True),  \n",
    "    StructField(\"usage\", StringType(), True),\n",
    "    StructField(\"productDisplayName\", StringType(), True),\n",
    "    StructField(\"category_id\", IntegerType(), True),\n",
    "])\n",
    "\n",
    "\n",
    "transaction_schema = StructType([\n",
    "    StructField(\"created_at\", TimestampType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"booking_id\", IntegerType(), True),\n",
    "    StructField(\"session_id\", StringType(), True),  \n",
    "    StructField(\"product_metadata\", StringType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "    StructField(\"payment_status\", StringType(), True),\n",
    "    StructField(\"promo_amount\", DoubleType(), True),\n",
    "    StructField(\"promo_code\", StringType(), True),\n",
    "    StructField(\"shipment_fee\", DoubleType(), True),\n",
    "    StructField(\"shipment_date_limit\", TimestampType(), True),\n",
    "    StructField(\"shipment_location_lat\", DoubleType(), True),\n",
    "    StructField(\"shipment_location_long\", DoubleType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category DataFrame :\n",
      "root\n",
      " |-- category_id: integer (nullable = true)\n",
      " |-- cat_level1: string (nullable = true)\n",
      " |-- cat_level2: string (nullable = true)\n",
      " |-- cat_level3: string (nullable = true)\n",
      "\n",
      "Customer DataFrame :\n",
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- username: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- birthdate: date (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      " |-- device_id: string (nullable = true)\n",
      " |-- device_version: string (nullable = true)\n",
      " |-- home_location_lat: double (nullable = true)\n",
      " |-- home_location_long: double (nullable = true)\n",
      " |-- home_location: string (nullable = true)\n",
      " |-- home_country: string (nullable = true)\n",
      " |-- first_join_date: date (nullable = true)\n",
      "\n",
      "Product DataFrame :\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- baseColour: string (nullable = true)\n",
      " |-- season: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- usage: string (nullable = true)\n",
      " |-- productDisplayName: string (nullable = true)\n",
      " |-- category_id: integer (nullable = true)\n",
      "\n",
      "Transaction DataFrame :\n",
      "root\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- booking_id: string (nullable = true)\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- product_metadata: string (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- payment_status: string (nullable = true)\n",
      " |-- promo_amount: integer (nullable = true)\n",
      " |-- promo_code: string (nullable = true)\n",
      " |-- shipment_fee: integer (nullable = true)\n",
      " |-- shipment_date_limit: timestamp (nullable = true)\n",
      " |-- shipment_location_lat: double (nullable = true)\n",
      " |-- shipment_location_long: double (nullable = true)\n",
      " |-- total_amount: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "category_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"category.csv\")\n",
    "customer_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"customer.csv\")\n",
    "product_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"product.csv\")\n",
    "transaction_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"new_transactions.csv\")\n",
    "\n",
    "\n",
    "print(\"Category DataFrame :\")\n",
    "category_df = category_df.drop(\"#\")\n",
    "category_df.printSchema()\n",
    "\n",
    "print(\"Customer DataFrame :\")\n",
    "customer_df = customer_df.drop(\"#\")\n",
    "customer_df.printSchema()\n",
    "\n",
    "print(\"Product DataFrame :\")\n",
    "product_df = product_df.drop(\"#\")\n",
    "product_df.printSchema()\n",
    "\n",
    "print(\"Transaction DataFrame :\")\n",
    "transaction_df= transaction_df.drop(\"#\")\n",
    "transaction_df.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Using the Kafka topic from the producer in Task 1, ingest the streaming data into Spark Streaming, assuming all data comes in the String format. Except for the 'ts' column, you shall receive it as an Int type.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Setting up a structured streaming job to consume data from a Kafka topic, apply schema and data transformations, and write the results to the console.\n",
    " a) Reading streaming data from a Kafka topic named 'ass2a' \n",
    " b) Specifying \"startingOffsets\" to \"latest,\" to read from the latest available messages \n",
    " c)This schema describes the expected structure of the data (where \"ts\" field is expected as an Integer)\n",
    " d)Using from_json function to parse the JSON data in the \"value\" column based on the schema you defined. \n",
    " e)Using explode function to flatten the array of structs created by from_json.\n",
    " d)Renaming columns to match the expected column names.\n",
    " e)Then configuring the streaming query to write the transformed data to the console.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_unixtime, col\n",
    "hostip = \"172.20.10.5\" \n",
    "topic = 'ass2a'\n",
    "\n",
    "\n",
    "kafka_stream_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", f'{hostip}:9092') \\\n",
    "    .option(\"subscribe\", topic) \\\n",
    "    .option(\"startingOffsets\", \"latest\")\\\n",
    "    .option(\"failOnDataLoss\", \"false\")\\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"key\" and \"value\" columns in the Kafka DataFrame are explicitly cast to strings.\n",
    "kafka_stream_df = kafka_stream_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- parsed_value: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- id: string (nullable = true)\n",
      " |    |    |-- session_id: string (nullable = true)\n",
      " |    |    |-- event_name: string (nullable = true)\n",
      " |    |    |-- event_id: string (nullable = true)\n",
      " |    |    |-- traffic_source: string (nullable = true)\n",
      " |    |    |-- event_metadata: string (nullable = true)\n",
      " |    |    |-- customer_id: string (nullable = true)\n",
      " |    |    |-- ts: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = ArrayType(StructType([    \n",
    "    StructField(\"id\", StringType(),True),\n",
    "    StructField(\"session_id\", StringType(),True),\n",
    "    StructField(\"event_name\", StringType(),True),\n",
    "    StructField(\"event_id\", StringType(),True),\n",
    "    StructField(\"traffic_source\", StringType(),True),\n",
    "    StructField(\"event_metadata\", StringType(),True),\n",
    "    StructField(\"customer_id\", StringType(),True),\n",
    "    StructField(\"ts\", IntegerType(),True)\n",
    "]))\n",
    "kafka_stream_df = kafka_stream_df.select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias('parsed_value'))\n",
    "kafka_stream_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- unnested_value: struct (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- session_id: string (nullable = true)\n",
      " |    |-- event_name: string (nullable = true)\n",
      " |    |-- event_id: string (nullable = true)\n",
      " |    |-- traffic_source: string (nullable = true)\n",
      " |    |-- event_metadata: string (nullable = true)\n",
      " |    |-- customer_id: string (nullable = true)\n",
      " |    |-- ts: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_stream_df = kafka_stream_df.select(F.explode(F.col(\"parsed_value\")).alias('unnested_value'))      \n",
    "kafka_stream_df .printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_stream_df_formatted = kafka_stream_df.select(\n",
    "                     F.col(\"unnested_value.id\").alias(\"Id\"),\n",
    "                    F.col(\"unnested_value.session_id\").alias(\"session_id\"),\n",
    "                    F.col(\"unnested_value.event_name\").alias(\"event_name\"),\n",
    "                    F.col(\"unnested_value.event_id\").alias(\"event_id\"),\n",
    "                    F.col(\"unnested_value.traffic_source\").alias(\"traffic_source\"),\n",
    "                    F.col(\"unnested_value.event_metadata\").alias(\"event_metadata\"),\n",
    "                    F.col(\"unnested_value.customer_id\").alias(\"customer_id\"),\n",
    "                    F.col(\"unnested_value.ts\").alias(\"ts\")\n",
    "    \n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- event_name: string (nullable = true)\n",
      " |-- event_id: string (nullable = true)\n",
      " |-- traffic_source: string (nullable = true)\n",
      " |-- event_metadata: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- ts: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_stream_df_formatted.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = kafka_stream_df_formatted\\\n",
    "   .writeStream \\\n",
    "   .outputMode(\"update\") \\\n",
    "   .format(\"console\") \\\n",
    "   .trigger(processingTime=\"5 seconds\")\\\n",
    "   .start()\n",
    "\n",
    "#query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Then, the streaming data format should be transformed into the proper formats following the metadata file schema, similar to assignment 2A.  \n",
    "Perform the following tasks:  \n",
    "a) For the 'ts' column, convert it to the timestamp format, we will use it as event_time.  \n",
    "b) If the data is late for more than 1 minute, discard it.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "a) Transformations are applied to kafka_stream_df_formatted DataFrame also Columns are renamed and\n",
    "    cast to their desired data types.\n",
    "b) The \"ts\" column is converted to a timestamp format. The result is a new column named \"event_time.\"\n",
    "\n",
    "c) Filtering Late Data:\n",
    "        Data  more than 1 minute is discarded by calculating the time difference \n",
    "        between the current timestamp and the \"event_time,\"  and\n",
    "        filtering out rows where the difference is greater than 60 seconds.\n",
    "\n",
    "\n",
    "Windowed Aggregation:\n",
    "\n",
    "The code sets up a windowed aggregation using a 1-minute window on the \"event_time\" column.\n",
    "Counting no of sessions within each window \n",
    "\n",
    "\n",
    "A processing time trigger is set for the streaming query to execute every 2 minutes.\n",
    "Schema Output:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hostip = \"172.20.10.5\" \n",
    "topic = 'ass2a'\n",
    "\n",
    "\n",
    "kafka_stream_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", f'{hostip}:9092') \\\n",
    "    .option(\"subscribe\", topic) \\\n",
    "    .option(\"startingOffsets\", \"latest\")\\\n",
    "    .option(\"failOnDataLoss\", \"false\")\\\n",
    "    .load()\n",
    "\n",
    "kafka_stream_df = kafka_stream_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_stream_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- event_name: string (nullable = true)\n",
      " |-- event_id: string (nullable = true)\n",
      " |-- traffic_source: string (nullable = true)\n",
      " |-- event_metadata: string (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_stream_df_formatted = kafka_stream_df_formatted \\\n",
    "    .withColumn(\"id\", kafka_stream_df_formatted[\"Id\"].cast(IntegerType())) \\\n",
    "    .withColumn(\"session_id\", kafka_stream_df_formatted[\"session_id\"]) \\\n",
    "    .withColumn(\"event_name\", kafka_stream_df_formatted[\"event_name\"]) \\\n",
    "    .withColumn(\"event_id\", kafka_stream_df_formatted[\"event_id\"]) \\\n",
    "    .withColumn(\"traffic_source\", kafka_stream_df_formatted[\"traffic_source\"]) \\\n",
    "    .withColumn(\"event_metadata\", kafka_stream_df_formatted[\"event_metadata\"]) \\\n",
    "    .withColumn(\"customer_id\", kafka_stream_df_formatted[\"customer_id\"].cast(IntegerType())) \\\n",
    "    .withColumn(\"ts\", kafka_stream_df_formatted[\"ts\"].cast(LongType())) \\\n",
    "    .select(\"id\", \"session_id\", \"event_name\", \"event_id\", \"traffic_source\", \"event_metadata\", \"customer_id\", \"ts\")\n",
    "\n",
    "kafka_stream_df_formatted=kafka_stream_df_formatted.drop(\"id\")\n",
    "kafka_stream_df_formatted.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting 'ts' column to timestamp and rename it to 'event_time'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- event_name: string (nullable = true)\n",
      " |-- event_id: string (nullable = true)\n",
      " |-- traffic_source: string (nullable = true)\n",
      " |-- event_metadata: string (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_unixtime, col\n",
    "\n",
    "\n",
    "kafka_stream_df_formatted = kafka_stream_df_formatted.withColumn(\n",
    "    'event_time',\n",
    "    from_unixtime(col('ts').cast('double')).cast('timestamp')\n",
    ")\n",
    "kafka_stream_df_formatted = kafka_stream_df_formatted.drop('ts')\n",
    "kafka_stream_df_formatted.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "current_time = current_timestamp()\n",
    "kafka_stream_df_formatted = kafka_stream_df_formatted.filter((current_time - F.col(\"event_time\")).cast(\"int\") <= 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window, sum\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "windowedCounts = kafka_stream_df_formatted \\\n",
    "    .withWatermark(\"event_time\", \"1 minute\") \\\n",
    "    .groupBy(window(kafka_stream_df_formatted.event_time, \"1 minute\"))\\\n",
    "    .agg(F.count(\"session_id\").alias(\"total\"))\\\n",
    "    .select(\"window\",\"total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = kafka_stream_df_formatted \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    " \n",
    "#query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- event_name: string (nullable = true)\n",
      " |-- event_id: string (nullable = true)\n",
      " |-- traffic_source: string (nullable = true)\n",
      " |-- event_metadata: string (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_stream_df_formatted .printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5  \n",
    "Aggregate the streaming data frame by session id and create features you used in your assignment 2A model. (note: customer ID has already been included in the stream.)   \n",
    "Then, join the static data frames with the streaming data frame as our final data for prediction.  \n",
    "Perform data type/column conversion according to your ML model, and print out the Schema.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "processesing streaming data, to create features from it, and joins it with static customer information.\n",
    "The features include counts of different event categories, season,num_cat_highvalue,num_cat_midvaluenum_cat_lowvalue,total_Actions. \n",
    "\n",
    "Additionally, a \"made_purchase\" feature is added based on payment status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- event_name: string (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- event_metadata: string (nullable = true)\n",
      " |-- num_cat_highvalue: long (nullable = false)\n",
      " |-- num_cat_midvalue: long (nullable = false)\n",
      " |-- num_cat_lowvalue: long (nullable = false)\n",
      " |-- is_promotion: long (nullable = false)\n",
      " |-- season: string (nullable = false)\n",
      " |-- total_actions: long (nullable = false)\n",
      " |-- high_value_ratio: double (nullable = true)\n",
      " |-- low_value_ratio: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, when, lit, month\n",
    "\n",
    "# Define the conditions for each season based on the month\n",
    "\n",
    "season_conditions = {\n",
    "    \"Spring\": (month(\"event_time\").between(3, 5)),\n",
    "    \"Summer\": (month(\"event_time\").between(6, 8)),\n",
    "    \"Autumn\": (month(\"event_time\").between(9, 11)),\n",
    "    \"Winter\": (month(\"event_time\").isin([12, 1, 2]))\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "#defining conditions for category\n",
    "category_conditions = {\n",
    "    \"Category 1\": (col(\"event_name\").isin([\"ADD_PROMO\", \"ADD_TO_CART\"])),\n",
    "    \"Category 2\": (col(\"event_name\").isin([\"VIEW_PROMO\", \"VIEW_ITEM\", \"SEARCH\"])),\n",
    "    \"Category 3\": (col(\"event_name\").isin([\"SCROLL\", \"HOMEPAGE\", \"CLICK\"]))\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "#creating features \n",
    "feature_df = kafka_stream_df_formatted.select(\"session_id\", \"event_name\", \"event_time\",\"customer_id\",\"event_metadata\") \\\n",
    "    .groupBy(\"session_id\",\"event_name\", \"event_time\",\"customer_id\",\"event_metadata\") \\\n",
    "    .agg(\n",
    "        count(when(category_conditions[\"Category 1\"], True)).alias(\"num_cat_highvalue\"),\n",
    "        count(when(category_conditions[\"Category 2\"], True)).alias(\"num_cat_midvalue\"),\n",
    "        count(when(category_conditions[\"Category 3\"], True)).alias(\"num_cat_lowvalue\"),\n",
    "        count(when(col(\"event_name\") == \"ADD_PROMO\", True)).alias(\"is_promotion\"),\n",
    "        when(season_conditions[\"Spring\"], \"Spring\")\n",
    "            .when(season_conditions[\"Summer\"], \"Summer\")\n",
    "            .when(season_conditions[\"Autumn\"], \"Autumn\")\n",
    "            .when(season_conditions[\"Winter\"], \"Winter\")\n",
    "            .otherwise(\"Unknown\").alias(\"season\")\n",
    "    ) \\\n",
    "    .withColumn(\"total_actions\", col(\"num_cat_highvalue\") + col(\"num_cat_midvalue\") + col(\"num_cat_lowvalue\")) \\\n",
    "    .withColumn(\"high_value_ratio\", (col(\"num_cat_highvalue\") / col(\"total_actions\")) * 100) \\\n",
    "    .withColumn(\"low_value_ratio\", (col(\"num_cat_lowvalue\") / col(\"total_actions\")) * 100)\n",
    "\n",
    "\n",
    "\n",
    "feature_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- event_name: string (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- event_metadata: string (nullable = true)\n",
      " |-- num_cat_highvalue: long (nullable = false)\n",
      " |-- num_cat_midvalue: long (nullable = false)\n",
      " |-- num_cat_lowvalue: long (nullable = false)\n",
      " |-- is_promotion: long (nullable = false)\n",
      " |-- season: string (nullable = false)\n",
      " |-- total_actions: long (nullable = false)\n",
      " |-- high_value_ratio: double (nullable = true)\n",
      " |-- low_value_ratio: double (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      " |-- home_location: string (nullable = true)\n",
      " |-- first_join_year: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#joining customer_df and feature_df\n",
    "\n",
    "\n",
    "new_customer_df = customer_df.select(\n",
    "    \"customer_id\",\n",
    "    \"gender\",\n",
    "    \"device_type\",\n",
    "    \"home_location\",\n",
    "    year(\"first_join_date\").alias(\"first_join_year\"),\n",
    "    (year(current_date()) - year(\"birthdate\")).alias(\"age\")\n",
    ")\n",
    "feature_df = feature_df.join(new_customer_df, \"customer_id\", \"left\")\n",
    "feature_df.na.drop()\n",
    "feature_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- event_name: string (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- event_metadata: string (nullable = true)\n",
      " |-- num_cat_highvalue: long (nullable = false)\n",
      " |-- num_cat_midvalue: long (nullable = false)\n",
      " |-- num_cat_lowvalue: long (nullable = false)\n",
      " |-- is_promotion: long (nullable = false)\n",
      " |-- season: string (nullable = false)\n",
      " |-- total_actions: long (nullable = false)\n",
      " |-- high_value_ratio: double (nullable = true)\n",
      " |-- low_value_ratio: double (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      " |-- home_location: string (nullable = true)\n",
      " |-- first_join_year: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- made_purchase: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#adding made_purchase into features based on payment status\n",
    "\n",
    "\n",
    "\n",
    "purchased_df = transaction_df.withColumn(\n",
    "    \"made_purchase\",\n",
    "    when(col(\"payment_status\") == \"Success\", 1).otherwise(0)\n",
    ").select(\"session_id\", \"made_purchase\")\n",
    "\n",
    "feature_df = feature_df.join(purchased_df, \"session_id\", \"left\").fillna(0, subset=[\"made_purchase\"])\n",
    "feature_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = feature_df\\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start()\n",
    "    \n",
    "\n",
    "\n",
    "#query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 Load your ML model, and use the model to predict if each session will purchase according to the requirements below:\n",
    "a) Every 10 seconds, show the total number of potential sales transactions (prediction = 1) in the last 1 minute.   \n",
    "b) Every 30 seconds, show the total potential revenue in the last 30 seconds. “Potiential revenue” here is definded as: When prediction=1, extract customer shopping cart detail from metadata (sum of all items of ADD_TO_CART events).  \n",
    "c) Every 1 minute, show the top 10 best-selling products by total quantity. (note: No historical data is required, only the top 10 in each 1 minute window.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Loading the model and using it to make predictions on a feature data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pyspark.ml import PipelineModel\n",
    "loaded_model = PipelineModel.load(\"gbt_model\")\n",
    "\n",
    "\n",
    "\n",
    "if isinstance(loaded_model, PipelineModel):\n",
    "    print(\"Model loaded successfully.\")\n",
    "else:\n",
    "    print(\"Failed to load the model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- event_name: string (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- event_metadata: string (nullable = true)\n",
      " |-- num_cat_highvalue: long (nullable = false)\n",
      " |-- num_cat_midvalue: long (nullable = false)\n",
      " |-- num_cat_lowvalue: long (nullable = false)\n",
      " |-- is_promotion: long (nullable = false)\n",
      " |-- season: string (nullable = false)\n",
      " |-- total_actions: long (nullable = false)\n",
      " |-- high_value_ratio: double (nullable = true)\n",
      " |-- low_value_ratio: double (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      " |-- home_location: string (nullable = true)\n",
      " |-- first_join_year: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- made_purchase: integer (nullable = true)\n",
      " |-- season_index: double (nullable = false)\n",
      " |-- gender_index: double (nullable = false)\n",
      " |-- device_type_index: double (nullable = false)\n",
      " |-- home_location_index: double (nullable = false)\n",
      " |-- is_promotion_index: double (nullable = false)\n",
      " |-- label: double (nullable = false)\n",
      " |-- season_vec: vector (nullable = true)\n",
      " |-- gender_vec: vector (nullable = true)\n",
      " |-- device_type_vec: vector (nullable = true)\n",
      " |-- home_location_vec: vector (nullable = true)\n",
      " |-- is_promotion_vec: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_df = feature_df.dropna()\n",
    "predictions = loaded_model.transform(feature_df)\n",
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = predictions\\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start()\n",
    "    \n",
    "#query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating a new DataFrame sales_transaction by filtering the predictions DataFrame where prediction==1\n",
    "using window(column, windowDuration, slideDuration) to define the time windows\n",
    "calculating the count of records where the \"prediction\" column has a value of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6A\n",
    "sales_transaction = predictions.filter(col(\"prediction\") == 1) \\\n",
    ".groupBy(window(col(\"event_time\"), \"60 seconds\", \"10 seconds\")) \\\n",
    ".agg(count(col(\"prediction\")).alias(\"total_sales_transactions\")) \\\n",
    ".select(\"total_sales_transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.streaming.statefulOperator.checkCorrectness.enabled\", \"false\")\n",
    "query = sales_transaction\\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- total_sales_transactions: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_transaction.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the total potential revenue in the last 30 seconds for sessions where the prediction is equal to 1.\n",
    "\n",
    "a)Defining  schema for the event_metadata column, specifying the expected structure of the JSON data within it.\n",
    "\n",
    "b)When the event_name is \"ADD_TO_CART,\" parsing the data in the event_metadata column.\n",
    " \n",
    "c)Calculates the revenue for each session by multiplying the quantity and item_price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6B\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import col, when, from_json, sum\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "from pyspark.sql.window import Window\n",
    "schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"item_price\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "predictions = predictions.withColumn(\"cart_items\", when(col(\"event_name\") == \"ADD_TO_CART\", from_json(col(\"event_metadata\"), schema)))\n",
    "\n",
    "predictions = predictions.withColumn(\"revenue\",\n",
    "    when(col(\"event_name\") == \"ADD_TO_CART\",\n",
    "        col(\"cart_items.quantity\") * col(\"cart_items.item_price\")\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "revenue_df = predictions \\\n",
    "    .withWatermark(\"event_time\", \"30 seconds\") \\\n",
    "    .groupBy(window(\"event_time\", \"30 seconds\", \"30 seconds\")) \\\n",
    "    .agg(sum(col(\"revenue\")).alias(\"potential_revenue\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- potential_revenue: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "revenue_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.streaming.statefulOperator.checkCorrectness.enabled\", \"false\")\n",
    "query = revenue_df.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime=\"60 seconds\") \\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a)\"product_quantities\" is created by selecting product IDs and quantities from \"predictions.\"\n",
    "b)It calculates the total quantity per product by grouping and summing their quantities, resulting in a \"total_quantity\" ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6C\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"item_price\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "product_quantities = predictions.select(\"cart_items.product_id\", \"cart_items.quantity\")\n",
    "product_quantities = product_quantities.groupBy(\"product_id\").agg(sum(\"quantity\").alias(\"total_quantity\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.streaming.statefulOperator.checkCorrectness.enabled\", \"false\")\n",
    "query =product_quantities.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime=\"60 seconds\") \\\n",
    "    .start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7  \n",
    "a) Persist the prediction result along with cart metadata in parquet format; after that, read the parquet file and show the results to verify it is saved properly.  \n",
    "b) Persist the 30-second sales prediction in another parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7a\n",
    "prediction1 =predictions.filter(col(\"event_name\") == \"ADD_TO_CART\") \\\n",
    "    .select(\"session_id\", \"customer_id\", \"event_time\", \"event_metadata\", \"made_purchase\", \"prediction\")\n",
    "prediction1=prediction1.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query_file_sink = kafka_stream_df_formatted.writeStream.format(\"parquet\")\\\n",
    "        .outputMode(\"append\")\\\n",
    "        .option(\"path\", \"parquet/clickstream_df\")\\\n",
    "        .option(\"checkpointLocation\", \"parquet/clickstream_df/checkpoint\")\\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_file_sink.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "\n",
    "# Define the schema for the data\n",
    "schema_1 = StructType([\n",
    "    StructField(\"session_id\", StringType(), True),\n",
    "    StructField(\"event_name\", StringType(), True),\n",
    "    StructField(\"event_id\", StringType(), True),\n",
    "    StructField(\"traffic_source\", StringType(), True),\n",
    "    StructField(\"event_metadata\", StringType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"event_time\", TimestampType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- event_name: string (nullable = true)\n",
      " |-- event_id: string (nullable = true)\n",
      " |-- traffic_source: string (nullable = true)\n",
      " |-- event_metadata: string (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_file_sink_df = spark.read.schema(schema_1).parquet(\"parquet/clickstream_df\")\n",
    "query_file_sink_df.printSchema()\n",
    "#query_file_sink_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------------------+--------------+--------------------+-----------+-------------------+\n",
      "|          session_id| event_name|            event_id|traffic_source|      event_metadata|customer_id|         event_time|\n",
      "+--------------------+-----------+--------------------+--------------+--------------------+-----------+-------------------+\n",
      "|4046178d-c332-465...|      CLICK|0b56a505-60ec-470...|        MOBILE|                    |      43174|2023-10-19 09:47:54|\n",
      "|4046178d-c332-465...|   HOMEPAGE|cbd6642e-b475-4e6...|        MOBILE|                    |      43174|2023-10-19 09:47:55|\n",
      "|40487207-8d37-46f...|ADD_TO_CART|ad9fbf26-8ff5-4a9...|        MOBILE|{'product_id': 91...|       4885|2023-10-19 09:47:56|\n",
      "|40487207-8d37-46f...|      CLICK|8f5cf8cc-8ece-41e...|        MOBILE|                    |       4885|2023-10-19 09:47:57|\n",
      "|40487207-8d37-46f...|   PURCHASE|2f50c8f6-f9fc-4e9...|        MOBILE|{'payment_status'...|       4885|2023-10-19 09:47:58|\n",
      "|40487207-8d37-46f...|   HOMEPAGE|100f10e7-75c6-4c0...|        MOBILE|                    |       4885|2023-10-19 09:47:59|\n",
      "|4048972a-db94-445...|   HOMEPAGE|963ab6ee-a662-434...|        MOBILE|                    |      34690|2023-10-19 09:48:00|\n",
      "|4048972a-db94-445...|     SEARCH|f84d3c85-43a3-484...|        MOBILE|{'search_keywords...|      34690|2023-10-19 09:48:01|\n",
      "|4048972a-db94-445...|   PURCHASE|4d284d84-eddb-4d8...|        MOBILE|{'payment_status'...|      34690|2023-10-19 09:48:02|\n",
      "|4048972a-db94-445...|     SEARCH|4131bb6b-721e-4e7...|        MOBILE|{'search_keywords...|      34690|2023-10-19 09:48:03|\n",
      "|4048972a-db94-445...|  VIEW_ITEM|7f5751d6-5e4e-450...|        MOBILE|                    |      34690|2023-10-19 09:48:04|\n",
      "|4048972a-db94-445...|     SEARCH|8c88b40e-1b22-496...|        MOBILE|{'search_keywords...|      34690|2023-10-19 09:48:05|\n",
      "|4048972a-db94-445...|   HOMEPAGE|b94811d0-e5ef-4a3...|        MOBILE|                    |      34690|2023-10-19 09:48:06|\n",
      "|4048972a-db94-445...|  VIEW_ITEM|19840808-efca-469...|        MOBILE|                    |      34690|2023-10-19 09:48:07|\n",
      "|4048972a-db94-445...|      CLICK|921ebbf2-a012-45b...|        MOBILE|                    |      34690|2023-10-19 09:48:08|\n",
      "|4048972a-db94-445...|ADD_TO_CART|0ce02339-04e3-452...|        MOBILE|{'product_id': 24...|      34690|2023-10-19 09:48:09|\n",
      "|4048972a-db94-445...|     SCROLL|9135c154-c1bc-4e6...|        MOBILE|                    |      34690|2023-10-19 09:48:10|\n",
      "|4048972a-db94-445...|      CLICK|644499de-35dc-4be...|        MOBILE|                    |      34690|2023-10-19 09:48:11|\n",
      "|4048972a-db94-445...|      CLICK|7a5435ac-cf3b-416...|        MOBILE|                    |      34690|2023-10-19 09:48:12|\n",
      "|4048972a-db94-445...|      CLICK|3c77b41a-7544-4e3...|        MOBILE|                    |      34690|2023-10-19 09:48:13|\n",
      "+--------------------+-----------+--------------------+--------------+--------------------+-----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_file_sink_df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7B\n",
    "predictions = predictions.withColumn(\"revenue\",\n",
    "    when(col(\"event_name\") == \"ADD_TO_CART\",\n",
    "        col(\"cart_items.quantity\") * col(\"cart_items.item_price\")\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "revenue_df = predictions \\\n",
    "    .withWatermark(\"event_time\", \"30 seconds\") \\\n",
    "    .groupBy(window(\"event_time\", \"30 seconds\", \"30 seconds\")) \\\n",
    "    .agg(sum(col(\"revenue\")).alias(\"potential_revenue\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Parquet path for sales predictions\n",
    "parquet_path = \"parquet/sales_predictions\"\n",
    "\n",
    "# Write the windowed results to Parquet\n",
    "query = revenue_df.writeStream.format(\"parquet\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"path\", parquet_path) \\\n",
    "    .option(\"checkpointLocation\", \"parquet/sales_predictions/checkpoint\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, TimestampType, DoubleType\n",
    "\n",
    "schema_3 = StructType([\n",
    "    StructField(\"window\", StructType([\n",
    "        StructField(\"start\", TimestampType(), True),\n",
    "        StructField(\"end\", TimestampType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"potential_revenue\", DoubleType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- window: struct (nullable = true)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- potential_revenue: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_file_sink_df = spark.read.schema(schema_3).parquet(\"parquet/clickstream_df\")\n",
    "query_file_sink_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8  \n",
    "Read the parquet files as a data stream, for 7a) join customer information and send to a Kafka topic with an appropriate name to the data visualisation. For 7b) Send the message directly to another Kafka topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream 2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
